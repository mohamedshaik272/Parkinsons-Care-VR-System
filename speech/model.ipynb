{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Preprocessing and Clustering\n",
    "\n",
    "Feature Extraction: We start by loading all voice .wav files from the healthy control and Parkinson’s patient folders. For each audio sample, we compute a range of acoustic features: Mel-Frequency Cepstral Coefficients (MFCCs) capturing spectral shape, jitter (cycle-to-cycle pitch variability), shimmer (amplitude variability), pitch (fundamental frequency) statistics, spectral entropy, and Harmonic-to-Noise Ratio (HNR). These features are commonly used to characterize voice pathologies – for example, fundamental frequency, jitter, shimmer, HNR, and MFCCs have been extracted for disease classification in prior studies ￼. In PD patients, such vocal features often show higher fluctuations and noise compared to healthy voices ￼.\n",
    "\n",
    "Unsupervised Severity Clustering: Since true severity labels may be unavailable, we apply unsupervised clustering to the patient feature vectors to derive severity levels. We use K-Means (k=4) to cluster the patient samples into 4 groups (intended to correspond to increasing severity 1–4). All healthy control samples are assigned label 0 (no disease). We map the cluster indices to severity scores 1–4 by ordering clusters based on an indicator (e.g. average jitter, assuming higher jitter corresponds to more severe dysphonia). The code below performs feature extraction and clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Paths to data folders (adjust as needed)\n",
    "healthy_dir = \"HealthyAudio/\"\n",
    "patient_dir = \"PatientAudio/\"\n",
    "\n",
    "# Feature extraction function for one audio signal\n",
    "def extract_features(y, sr):\n",
    "    features = []\n",
    "    # 1. MFCC features\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfcc_means = mfcc.mean(axis=1)\n",
    "    mfcc_stds = mfcc.std(axis=1)\n",
    "    features.extend(mfcc_means)\n",
    "    features.extend(mfcc_stds)\n",
    "    # 2. Fundamental frequency (pitch) using librosa's pitch tracking\n",
    "    f0, voiced_flag, _ = librosa.pyin(y, sr=sr, fmin=50, fmax=500)\n",
    "    f0 = f0[voiced_flag] if voiced_flag is not None else f0  # filter voiced frames\n",
    "    if f0.size > 0:\n",
    "        pitch_mean = float(np.nanmean(f0))\n",
    "        pitch_std = float(np.nanstd(f0))\n",
    "    else:\n",
    "        pitch_mean, pitch_std = 0.0, 0.0\n",
    "    # Jitter: relative average change in pitch per frame (proxy for cycle variability)\n",
    "    if f0.size > 1:\n",
    "        jitter = float(np.mean(np.abs(np.diff(f0))) / (np.mean(f0) + 1e-8))\n",
    "    else:\n",
    "        jitter = 0.0\n",
    "    features.append(jitter)\n",
    "    # 3. Shimmer: relative average change in amplitude (use RMS energy as proxy for amplitude)\n",
    "    rms = librosa.feature.rms(y=y)[0]\n",
    "    if rms.size > 1:\n",
    "        shimmer = float(np.mean(np.abs(np.diff(rms))) / (np.mean(rms) + 1e-8))\n",
    "    else:\n",
    "        shimmer = 0.0\n",
    "    features.append(shimmer)\n",
    "    # Include pitch stats as features as well\n",
    "    features.append(pitch_mean)\n",
    "    features.append(pitch_std)\n",
    "    # 4. Spectral entropy (measure of spectral flatness/disorder)\n",
    "    # Compute power spectrum and normalize to a probability distribution\n",
    "    D = np.abs(librosa.stft(y, n_fft=2048, hop_length=512))**2\n",
    "    ps = D.sum(axis=1)\n",
    "    ps_norm = ps / (ps.sum() + 1e-12)\n",
    "    spectral_entropy = -np.sum(ps_norm * np.log2(ps_norm + 1e-12))\n",
    "    features.append(spectral_entropy)\n",
    "    # 5. Harmonic-to-Noise Ratio (HNR) using harmonic-percussive source separation\n",
    "    harmonic, percussive = librosa.effects.hpss(y)\n",
    "    harm_energy = np.sum(harmonic**2)\n",
    "    noise_energy = np.sum(percussive**2)\n",
    "    HNR = 10 * np.log10((harm_energy + 1e-8) / (noise_energy + 1e-8))\n",
    "    features.append(HNR)\n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "# Load data and extract features for all samples\n",
    "X_features = []   # feature vectors\n",
    "y_labels = []     # labels (0 for healthy, temporary for patients)\n",
    "file_paths = []   # store file paths to maintain order\n",
    "\n",
    "# Process healthy controls (label 0)\n",
    "for filepath in glob.glob(os.path.join(healthy_dir, \"*.wav\")):\n",
    "    y, sr = librosa.load(filepath, sr=None)  # use original sampling rate\n",
    "    feats = extract_features(y, sr)\n",
    "    X_features.append(feats)\n",
    "    y_labels.append(0)  # healthy label 0\n",
    "    file_paths.append(os.path.basename(filepath))\n",
    "\n",
    "# Process patient samples (temporary label -1 to distinguish before clustering)\n",
    "patient_feats = []\n",
    "patient_paths = []\n",
    "for filepath in glob.glob(os.path.join(patient_dir, \"*.wav\")):\n",
    "    y, sr = librosa.load(filepath, sr=None)\n",
    "    feats = extract_features(y, sr)\n",
    "    patient_feats.append(feats)\n",
    "    patient_paths.append(os.path.basename(filepath))\n",
    "# (We'll assign actual labels after clustering)\n",
    " \n",
    "# Scale features for clustering and modeling\n",
    "X_all = np.vstack([np.array(X_features), np.array(patient_feats)])\n",
    "scaler = StandardScaler()\n",
    "X_all_scaled = scaler.fit_transform(X_all)\n",
    "# Split back out the scaled features for patients and healthy\n",
    "X_h_scaled = X_all_scaled[:len(X_features)]\n",
    "X_p_scaled = X_all_scaled[len(X_features):]\n",
    "\n",
    "# Cluster the patient samples into 4 clusters (severity 1-4)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_p_scaled)\n",
    "# Determine severity order by average jitter in each cluster\n",
    "patient_feats_arr = np.array(patient_feats)\n",
    "jitter_index = 26  # index of jitter feature in our feature vector\n",
    "avg_jitter = []\n",
    "for k in range(4):\n",
    "    avg_jitter.append(patient_feats_arr[cluster_labels == k, jitter_index].mean())\n",
    "# Rank clusters by jitter (low jitter = mild, high jitter = severe)\n",
    "cluster_order = np.argsort(avg_jitter)\n",
    "# Map original cluster labels to severity 1-4\n",
    "cluster_to_severity = {int(cluster_order[i]): i+1 for i in range(4)}\n",
    "severity_labels = [cluster_to_severity[int(c)] for c in cluster_labels]\n",
    "\n",
    "# Combine healthy and patient labels\n",
    "y_patient = severity_labels  # severity 1-4 for each patient sample\n",
    "y_labels.extend(y_patient)   # 0 for all healthy (already in list) and 1-4 for patients\n",
    "X_features.extend(patient_feats)\n",
    "file_paths.extend(patient_paths)\n",
    "\n",
    "# Convert to numpy arrays for modeling\n",
    "X_features = np.array(X_features, dtype=np.float32)\n",
    "y_labels = np.array(y_labels, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code loads each audio file, computes a feature vector, and then uses KMeans to cluster patient samples into 4 groups. The cluster with the smallest average jitter is labeled as severity 1 (mildest) and the highest jitter cluster as severity 4 (most severe), with intermediate clusters labeled 2 and 3. All healthy controls are labeled 0. At the end, we have X_features (feature matrix) and y_labels (0–4 severity labels) for supervised model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Feature-Based Machine Learning Model\n",
    "\n",
    "With features and pseudo-severity labels prepared, we train traditional machine learning classifiers. We consider a Support Vector Machine (SVM) and a Random Forest (RF) model. SVMs are effective for high-dimensional feature spaces and often outperform neural networks when data is limited ￼, while Random Forests are robust ensemble classifiers that handle feature interactions well. Both models will learn to classify each sample into severity classes 0–4 based on the extracted acoustic features.\n",
    "\n",
    "We split the data into training and testing sets (e.g., 80/20 split) and train the classifiers. After training, we evaluate their accuracy and examine the confusion matrix. Code for training and evaluating the SVM and RF models is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.5882352941176471\n",
      "RF Accuracy: 0.5882352941176471\n",
      "\n",
      "SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72         9\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.59        17\n",
      "   macro avg       0.31      0.30      0.28        17\n",
      "weighted avg       0.42      0.59      0.46        17\n",
      "\n",
      "SVM Confusion Matrix:\n",
      " [[9 0 0 0 0]\n",
      " [2 0 0 0 0]\n",
      " [3 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [1 0 0 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Split data into train and test sets (stratified by severity to preserve class proportions)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.2, stratify=y_labels, random_state=42)\n",
    "\n",
    "# Train SVM classifier\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "# Train Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_pred))\n",
    "print(\"RF Accuracy:\", accuracy_score(y_test, rf_pred))\n",
    "print(\"\\nSVM Classification Report:\\n\", classification_report(y_test, svm_pred))\n",
    "print(\"SVM Confusion Matrix:\\n\", confusion_matrix(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trains an RBF-kernel SVM and a Random Forest. We output the accuracy and a classification report for the SVM as an example (the RF can be evaluated similarly). The classification report includes precision, recall (sensitivity), and F1-score for each class, and the confusion matrix shows how predicted labels align with true labels. We would typically see lower severity classes (including healthy 0) being confused with adjacent classes if the features are similar, while very distinct classes separate better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Deep Learning Model (Spectrogram-Based)\n",
    "\n",
    "To capture more complex patterns in the audio, we use a deep learning approach on the raw audio signals. We first convert each .wav file into a Mel spectrogram, which is a time-frequency representation of the audio. The spectrogram can be treated as an image (with time on one axis, frequency on the other, and intensity as pixel values) and fed into a Convolutional Neural Network (CNN) ￼. The CNN can automatically learn salient features (such as tremor or noise patterns) from the spectrograms that correlate with Parkinson’s severity.\n",
    "\n",
    "Below, we compute mel spectrograms for all samples and build a CNN model. Each spectrogram is converted to decibel (log) scale and padded to the same dimension. The CNN consists of convolutional and pooling layers to extract spatial features from the spectrogram, followed by dense layers to output a severity class. We train the CNN on the training set spectrograms. Such spectrogram-based CNN classifiers can capture subtle voice characteristics and have been shown to outperform classical feature-based methods for PD detection ￼.\n",
    "\n",
    "Additionally, we demonstrate how to apply transfer learning to improve the deep model. This involves using a pre-trained CNN (e.g. ResNet50 trained on ImageNet) as a fixed feature extractor for our spectrogram images, or fine-tuning it on our data ￼. We also mention using a pre-trained audio model like Wav2Vec2 for transfer learning on raw waveform. The code below covers spectrogram generation, CNN training, and an optional transfer learning setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 588ms/step - accuracy: 0.3407 - loss: 52.9768 - val_accuracy: 0.0769 - val_loss: 23.3810\n",
      "Epoch 2/30\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277ms/step - accuracy: 0.1748 - loss: 47.4927 - val_accuracy: 0.6154 - val_loss: 2.9664\n",
      "Epoch 3/30\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257ms/step - accuracy: 0.2875 - loss: 10.5067 - val_accuracy: 0.2308 - val_loss: 1.7315\n",
      "Epoch 4/30\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 487ms/step - accuracy: 0.3730 - loss: 2.1007 - val_accuracy: 0.2308 - val_loss: 1.5057\n",
      "Epoch 5/30\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 434ms/step - accuracy: 0.4877 - loss: 1.2269 - val_accuracy: 0.6154 - val_loss: 1.2148\n",
      "Epoch 6/30\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 452ms/step - accuracy: 0.5947 - loss: 1.1251 - val_accuracy: 0.3077 - val_loss: 1.4780\n",
      "Epoch 7/30\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 965ms/step - accuracy: 0.6891 - loss: 0.9737 - val_accuracy: 0.5385 - val_loss: 1.3579\n",
      "Epoch 8/30\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 462ms/step - accuracy: 0.6707 - loss: 1.0257 - val_accuracy: 0.5385 - val_loss: 1.4610\n",
      "CNN Test Accuracy: 0.29411765933036804\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "CNN Confusion Matrix:\n",
      " [[4 1 3 0 1]\n",
      " [1 0 1 0 0]\n",
      " [1 0 1 0 1]\n",
      " [0 0 1 0 0]\n",
      " [2 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Convert all audio files to Mel spectrograms (with consistent shape)\n",
    "spectrograms = []\n",
    "n_mels = 128\n",
    "# Use the same file order as X_features/y_labels (file_paths list was built above)\n",
    "for filepath in file_paths:\n",
    "    # We need the full path; assuming healthy_dir and patient_dir contain unique filenames, determine prefix:\n",
    "    if filepath in os.listdir(healthy_dir):\n",
    "        full_path = os.path.join(healthy_dir, filepath)\n",
    "    else:\n",
    "        full_path = os.path.join(patient_dir, filepath)\n",
    "    y, sr = librosa.load(full_path, sr=None)\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=512, n_fft=1024)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    spectrograms.append(S_db)\n",
    "\n",
    "# Pad all spectrograms to the same time length\n",
    "max_frames = max([spec.shape[1] for spec in spectrograms])\n",
    "spectrograms_padded = []\n",
    "for spec in spectrograms:\n",
    "    if spec.shape[1] < max_frames:\n",
    "        # pad with zeros (silence) to match max_frames\n",
    "        pad_width = max_frames - spec.shape[1]\n",
    "        spec_padded = np.pad(spec, ((0,0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        spec_padded = spec[:, :max_frames]  # truncate if somehow longer\n",
    "    spectrograms_padded.append(spec_padded)\n",
    "spectrograms_padded = np.array(spectrograms_padded, dtype=np.float32)\n",
    "# Add channel dimension for CNN input\n",
    "X_spec = spectrograms_padded[..., np.newaxis]  # shape (num_samples, n_mels, max_frames, 1)\n",
    "\n",
    "# Split into train/test sets (same y_labels as before)\n",
    "X_train_spec, X_test_spec, y_train_spec, y_test_spec = train_test_split(X_spec, y_labels, test_size=0.2, stratify=y_labels, random_state=42)\n",
    "\n",
    "# Define a CNN model for spectrogram classification\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(n_mels, max_frames, 1)),\n",
    "    layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    layers.Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(5, activation='softmax')  # 5 classes: 0,1,2,3,4\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN model\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
    "history = model.fit(X_train_spec, y_train_spec, epochs=30, batch_size=16, \n",
    "                    validation_split=0.2, callbacks=callbacks, verbose=1)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test_spec, y_test_spec, verbose=0)\n",
    "print(\"CNN Test Accuracy:\", test_acc)\n",
    "print(\"CNN Confusion Matrix:\\n\", confusion_matrix(y_test_spec, np.argmax(model.predict(X_test_spec), axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN is trained on the spectrogram images with an early stopping callback to prevent overfitting. After training, we evaluate the accuracy on the test set and show a confusion matrix of predicted vs actual severity classes.\n",
    "\n",
    "Transfer Learning (Optional): We can improve the CNN by leveraging a pre-trained network. For example, we can use ResNet50 (pre-trained on ImageNet) to extract features from our spectrograms. Since ResNet expects 3-channel images, we replicate the single-channel spectrogram into an RGB image and resize to the input shape (e.g., 224×224). We then use ResNet’s convolutional base to process the spectrogram and add a new classifier layer on top for our 5 classes. We freeze the pre-trained layers initially and train the top layer on our data, optionally unfreezing some layers later for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of setting up a transfer learning model with ResNet50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9s/step - accuracy: 0.1979 - loss: 1.7063 - val_accuracy: 0.0000e+00 - val_loss: 2.6784\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.7044 - loss: 1.0628"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m transfer_model.compile(loss=\u001b[33m'\u001b[39m\u001b[33msparse_categorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m, optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m, metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Train the transfer learning model on our spectrogram data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mtransfer_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_spec_resized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:395\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_eval_epoch_iterator\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m._eval_epoch_iterator = TFEpochIterator(\n\u001b[32m    386\u001b[39m         x=val_x,\n\u001b[32m    387\u001b[39m         y=val_y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    393\u001b[39m         shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    394\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m val_logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m val_logs = {\n\u001b[32m    406\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mval_\u001b[39m\u001b[33m\"\u001b[39m + name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs.items()\n\u001b[32m    407\u001b[39m }\n\u001b[32m    408\u001b[39m epoch_logs.update(val_logs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:483\u001b[39m, in \u001b[36mTensorFlowTrainer.evaluate\u001b[39m\u001b[34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[39m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    482\u001b[39m     callbacks.on_test_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m     callbacks.on_test_batch_end(step, logs)\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_evaluating:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Expand spectrogram data to 3 channels by duplicating the single channel\n",
    "X_spec_rgb = np.repeat(X_spec, 3, axis=-1)  # shape: (samples, n_mels, max_frames, 3)\n",
    "# If required, resize spectrograms to 224x224 for ResNet (using TensorFlow image resizing)\n",
    "target_size = 224\n",
    "X_spec_resized = tf.image.resize(X_spec_rgb, [target_size, target_size]).numpy()\n",
    "\n",
    "# Load pre-trained ResNet50 without its top layer\n",
    "base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, \n",
    "                                           input_shape=(target_size, target_size, 3))\n",
    "base_model.trainable = False  # freeze convolutional layers\n",
    "\n",
    "# Add a new classification head on top of ResNet\n",
    "transfer_model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(5, activation='softmax')\n",
    "])\n",
    "transfer_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the transfer learning model on our spectrogram data\n",
    "transfer_model.fit(X_spec_resized, y_labels, epochs=10, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this snippet, we prepare the spectrogram data for ResNet and then create a model that uses ResNet50’s convolutional layers (frozen) followed by a global pooling and a dense output layer. We would train this model similarly (with a validation split for early stopping). Using a pre-trained CNN can jump-start learning by applying features learned from natural images to our spectrograms ￼."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, for raw audio, one could fine-tune a pre-trained audio transformer like Wav2Vec2. For example, using HuggingFace Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Pseudo-code for Wav2Vec2 fine-tuning)\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "model_wav2vec = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    \"superb/wav2vec2-base-superb-ks\", num_labels=5)\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"superb/wav2vec2-base-superb-ks\")\n",
    "\n",
    "# Prepare audio data for Wav2Vec2\n",
    "input_values = processor([librosa.resample(y, orig_sr=sr, target_sr=16000) for y in audio_signals],\n",
    "                          sampling_rate=16000, return_tensors=\"np\", padding=True).input_values\n",
    "# Fine-tune model (this requires a training loop or Trainer API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This illustrates how to set up a deep learning model using spectrogram images and how to incorporate transfer learning. After training, the CNN-based model will output probabilities for each class 0–4 given a spectrogram input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Hybrid Model (Fusion of ML and Deep Learning)\n",
    "\n",
    "The hybrid approach combines the best of both worlds: the hand-crafted acoustic features and the deep learning derived features. We first use the trained CNN (from step 3) to extract high-level representations of each sample – for instance, taking the activations from the penultimate layer of the CNN as an embedding. This embedding captures complex voice patterns learned by the CNN. We then concatenate this embedding with the original acoustic feature vector (from step 1) for each sample. The fused feature (a longer vector) is used to train a meta-classifier (here we can use a Random Forest or another SVM) that predicts the severity. The rationale is that the classifier can utilize both the domain-specific features (jitter, MFCCs, etc.) and the CNN’s automatically learned features. Similar feature-fusion strategies (e.g., using a pre-trained CNN’s features with an auxiliary classifier) have improved accuracy in voice disorder classification.\n",
    "\n",
    "Below, we obtain CNN embeddings for our train/test sets and train a Random Forest on the concatenated features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "The layer sequential_2 has never been called and thus has no defined input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m _ = model.predict(X_train_spec[:\u001b[32m1\u001b[39m])\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Now you can safely create a feature extractor\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m feature_extractor = Model(inputs=\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m, outputs=model.layers[-\u001b[32m2\u001b[39m].output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.12/site-packages/keras/src/ops/operation.py:268\u001b[39m, in \u001b[36mOperation.input\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Retrieves the input tensor(s) of a symbolic operation.\u001b[39;00m\n\u001b[32m    261\u001b[39m \n\u001b[32m    262\u001b[39m \u001b[33;03m    Only returns the tensor(s) corresponding to the *first time*\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    266\u001b[39m \u001b[33;03m        Input tensor or list of input tensors.\u001b[39;00m\n\u001b[32m    267\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_node_attribute_at_index\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_tensors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.12/site-packages/keras/src/ops/operation.py:299\u001b[39m, in \u001b[36mOperation._get_node_attribute_at_index\u001b[39m\u001b[34m(self, node_index, attr, attr_name)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\u001b[39;00m\n\u001b[32m    284\u001b[39m \n\u001b[32m    285\u001b[39m \u001b[33;03mThis is used to implement the properties:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    296\u001b[39m \u001b[33;03m    The operation's attribute `attr` at the node of index `node_index`.\u001b[39;00m\n\u001b[32m    297\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inbound_nodes:\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m    300\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has never been called \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    301\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mand thus has no defined \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    302\u001b[39m     )\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._inbound_nodes) > node_index:\n\u001b[32m    304\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    305\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAsked to get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m at node \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    306\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but the operation has only \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._inbound_nodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m inbound nodes.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: The layer sequential_2 has never been called and thus has no defined input."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "_ = model.predict(X_train_spec[:1])  # Force a forward pass on a single sample\n",
    "\n",
    "# Use the trained CNN model from above to extract features (exclude final softmax layer)\n",
    "feature_extractor = Model(inputs=model.input, outputs=model.layers[-2].output)  # second last layer output\n",
    "\n",
    "# Extract CNN embeddings for training and test sets\n",
    "train_cnn_feat = feature_extractor.predict(X_train_spec)\n",
    "test_cnn_feat  = feature_extractor.predict(X_test_spec)\n",
    "\n",
    "# We also need the corresponding acoustic features for the same train/test split.\n",
    "# We can split X_features and y_labels in the same way (using the same indices as train_test_split earlier).\n",
    "X_train_feat, X_test_feat, _, _ = train_test_split(X_features, y_labels, test_size=0.2, stratify=y_labels, random_state=42)\n",
    "\n",
    "# Concatenate acoustic features with CNN features for fused representation\n",
    "X_train_fused = np.concatenate([X_train_feat, train_cnn_feat], axis=1)\n",
    "X_test_fused  = np.concatenate([X_test_feat, test_cnn_feat], axis=1)\n",
    "\n",
    "# Train a meta-classifier on the fused features (e.g., Random Forest)\n",
    "meta_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "meta_model.fit(X_train_fused, y_train_spec)  # y_train_spec is same as y_train\n",
    "\n",
    "# Evaluate hybrid model on test set\n",
    "hybrid_pred = meta_model.predict(X_test_fused)\n",
    "print(\"Hybrid Model Accuracy:\", accuracy_score(y_test_spec, hybrid_pred))\n",
    "print(\"Hybrid Confusion Matrix:\\n\", confusion_matrix(y_test_spec, hybrid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, feature_extractor uses the previously trained model (CNN) to output the 64-dimensional features from the layer before the final softmax. We generate these for each sample in the training and test sets. We then combine them with the original scaled features (X_train_feat, X_test_feat). Finally, we train a meta_model (here a Random Forest) on the fused features. We can evaluate its accuracy and confusion matrix on the test set. This hybrid classifier can often achieve better performance by leveraging both learned and engineered features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Incorporating Demographic Data (Age and Sex)\n",
    "\n",
    "Demographic factors such as age and sex can also influence voice characteristics (for example, females have about 1.5× higher fundamental frequency than males on average ￼). Parkinson’s patients in voice datasets also tend to be older than healthy controls ￼. To build a second version of our model that includes demographics, we simply add age and sex as additional features.\n",
    "\n",
    "For the feature-based models (SVM, RF, hybrid), this means appending the age and sex values to each sample’s feature vector. For sex, we encode it as a binary variable (e.g., 0 = Male, 1 = Female). Age can be used as a raw value or normalized. In the deep learning approach, one could incorporate demographics by using a multi-input network (one branch for the spectrogram, another for demographics) that merges before the final prediction, or more simply by adding these values into the fused feature vector in the hybrid model.\n",
    "\n",
    "Below, we show how to load an external demographics file and merge age/sex with the acoustic features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load demographic data (ensure the file path is correct)\u001b[39;00m\n\u001b[32m      4\u001b[39m demo_df = pd.read_excel(\u001b[33m\"\u001b[39m\u001b[33mdemographics.xlsx\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load demographic data (ensure the file path is correct)\n",
    "demo_df = pd.read_excel(\"demographics.xlsx\")\n",
    "# Assume demo_df has columns: \"Sample ID\", \"Label\" (HC or PwPD), \"Age\", \"Sex\"\n",
    "# Convert sex to numeric binary\n",
    "demo_df['Sex'] = demo_df['Sex'].map({'M': 0, 'F': 1})\n",
    "\n",
    "# Create a lookup dictionary for demographics by sample ID\n",
    "demo_info = {row['Sample ID']: (row['Age'], row['Sex']) for _, row in demo_df.iterrows()}\n",
    "\n",
    "# Append age and sex to feature vectors\n",
    "X_features_demo = []\n",
    "for i, filepath in enumerate(file_paths):\n",
    "    base_id = os.path.splitext(filepath)[0]  # sample ID without extension\n",
    "    feats = X_features[i]\n",
    "    age_val, sex_val = 0, 0\n",
    "    if base_id in demo_info:\n",
    "        age_val, sex_val = demo_info[base_id]\n",
    "    # concatenate original features with age and sex\n",
    "    feats_demo = np.concatenate([feats, [age_val, sex_val]], axis=0)\n",
    "    X_features_demo.append(feats_demo)\n",
    "X_features_demo = np.array(X_features_demo, dtype=np.float32)\n",
    "\n",
    "# Scale the extended features (age can be scaled, sex is 0/1 so scaling is optional)\n",
    "scaler_demo = StandardScaler()\n",
    "X_features_demo = scaler_demo.fit_transform(X_features_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read an Excel file containing age and sex for each sample (identified by an ID). We merge this info with our feature matrix X_features. The result X_features_demo includes the original acoustic features plus two extra dimensions for age and sex. We then normalize the features (note: for sex, since it’s binary, normalization isn’t strictly necessary, but including it won’t harm). Now we can reuse the previous modeling code with X_features_demo in place of X_features. For example, training the SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split with demographic-enhanced features\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_features_demo, y_labels, test_size=0.2, stratify=y_labels, random_state=42)\n",
    "svm_model_d = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_model_d.fit(X_train_d, y_train_d)\n",
    "print(\"SVM (with demographics) Accuracy:\", accuracy_score(y_test_d, svm_model_d.predict(X_test_d)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would follow similarly for RF or the hybrid model (concatenating the CNN embedding with the extended features). By comparing results, we can observe if including age and sex improves the performance. In many cases, adding demographics can subtly boost accuracy or help disambiguate certain cases (for instance, an older male voice might be more likely to be PD than a young female, if other features are borderline).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Model Evaluation and Deployment\n",
    "\n",
    "Cross-Validation and Performance Metrics\n",
    "\n",
    "To ensure our models generalize well, we perform cross-validation and compute detailed performance metrics. We can use k-fold cross-validation (e.g., 5-fold) on the training data to tune hyperparameters and estimate accuracy more robustly, avoiding overfitting to a single train/test split. For example, we could run 5-fold CV for the SVM’s C parameter and choose the value that gives highest mean accuracy. Below is how to get a cross-validated accuracy for the SVM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(svm_model, X_features, y_labels, cv=5)  # 5-fold CV on entire dataset\n",
    "print(\"SVM 5-fold CV Accuracy: %.2f%%\" % (100 * scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the final models, we evaluate them on a held-out test set. We calculate the confusion matrix and derive sensitivity and specificity for each class. Sensitivity (recall) for a given class is the proportion of that class correctly identified, and specificity for a class can be interpreted as the proportion of all other classes that are correctly identified as not being that class. In multi-class setting, we compute these per class. Here’s how we can compute sensitivity and specificity from a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Suppose we have true labels y_test and predictions y_pred from some model\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "num_classes = cm.shape[0]\n",
    "for i in range(num_classes):\n",
    "    TP = cm[i, i]\n",
    "    FN = cm[i, :].sum() - TP\n",
    "    FP = cm[:, i].sum() - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0.0  # recall for class i\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    print(f\"Class {i}: Sensitivity = {sensitivity:.2f}, Specificity = {specificity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output the sensitivity and specificity for each severity class 0 through 4. For instance, “Class 0” corresponds to healthy controls – sensitivity(0) is the true positive rate for detecting healthy voices, and specificity(0) is the rate at which patient voices are correctly identified as non-healthy. We would typically observe high sensitivity for class 0 if healthy voices are rarely misclassified, and high specificity for class 4 if severe cases are distinctly identified, etc. We also look at overall accuracy and perhaps macro-averaged metrics to summarize performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-Time Inference Function for Deployment\n",
    "\n",
    "Finally, we implement a real-time inference function to deploy the model. This function takes a new .wav file as input (and optionally the patient’s age/sex if using the demographic-enhanced model) and returns the predicted Parkinson’s severity level (0 to 4). It replicates the necessary preprocessing: loading the audio, extracting the same features and/or spectrogram, and then uses the trained model to output a class prediction. We ensure that the preprocessing (scaling, padding, etc.) is identical to what was done during training.\n",
    "\n",
    "Below is an example inference function that handles different model types (feature-based SVM, CNN, or hybrid). This assumes we have the trained models and scalers in scope (from the steps above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted severity for sample.wav: 2\n"
     ]
    }
   ],
   "source": [
    "def predict_severity(file_path, model_type='hybrid'):\n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    # Extract acoustic features\n",
    "    feats = extract_features(y, sr)  # reuse the same feature extraction function\n",
    "    # If the model includes demographics, one would obtain age/sex for this sample (e.g., via input or file lookup)\n",
    "    age_val, sex_val = 0, 0  # replace with actual data if available\n",
    "    if model_type in ['feature_demog', 'hybrid_demog']:\n",
    "        # include demographics in feature vector if required\n",
    "        feats = np.concatenate([feats, [age_val, sex_val]])\n",
    "        feats = scaler_demo.transform([feats])[0]\n",
    "    else:\n",
    "        feats = scaler.transform([feats])[0]\n",
    "    if model_type.startswith('feature'):\n",
    "        # Use feature-based ML model (SVM or RF)\n",
    "        if 'svm_model' in globals():\n",
    "            pred_class = int(svm_model.predict([feats])[0])\n",
    "        else:\n",
    "            pred_class = int(rf_model.predict([feats])[0])\n",
    "    elif model_type.startswith('cnn'):\n",
    "        # Use CNN model on spectrogram\n",
    "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=512, n_fft=1024)\n",
    "        S_db = librosa.power_to_db(S, ref=np.max)\n",
    "        # Pad or truncate to the same shape as training spectrograms\n",
    "        if S_db.shape[1] < max_frames:\n",
    "            S_db = np.pad(S_db, ((0,0), (0, max_frames - S_db.shape[1])), mode='constant')\n",
    "        else:\n",
    "            S_db = S_db[:, :max_frames]\n",
    "        X_input = S_db[np.newaxis, ..., np.newaxis]  # shape (1, n_mels, max_frames, 1)\n",
    "        probs = model.predict(X_input)\n",
    "        pred_class = int(np.argmax(probs, axis=1)[0])\n",
    "    else:  # hybrid\n",
    "        # Use hybrid model: combine features and CNN embedding then predict with meta-classifier\n",
    "        # Extract CNN embedding\n",
    "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=512, n_fft=1024)\n",
    "        S_db = librosa.power_to_db(S, ref=np.max)\n",
    "        if S_db.shape[1] < max_frames:\n",
    "            S_db = np.pad(S_db, ((0,0), (0, max_frames - S_db.shape[1])), mode='constant')\n",
    "        else:\n",
    "            S_db = S_db[:, :max_frames]\n",
    "        X_input = S_db[np.newaxis, ..., np.newaxis]\n",
    "        cnn_feat = feature_extractor.predict(X_input)[0]\n",
    "        fused_feat = np.concatenate([feats, cnn_feat])\n",
    "        pred_class = int(meta_model.predict([fused_feat])[0])\n",
    "    return pred_class\n",
    "\n",
    "# Example usage:\n",
    "new_file = \"sample.wav\"\n",
    "result = predict_severity(new_file, model_type='hybrid')\n",
    "print(f\"Predicted severity for {new_file}: {result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, model_type can be set to use the feature-based model ('feature' or 'feature_demog'), the pure CNN ('cnn'), or the hybrid ('hybrid' or 'hybrid_demog'). The function loads the audio, extracts features, and if needed computes the spectrogram and CNN embedding. It then applies the appropriate model to predict the class. For deployment, you would load the trained model objects (svm_model, model, meta_model, etc.) and the scalers from saved files, then use this function to get predictions on new data. The output is an integer 0–4 indicating the severity level, which could then be presented to clinicians or used in a downstream system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
