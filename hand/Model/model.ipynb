{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "from skimage.morphology import skeletonize, dilation, square\n",
    "\n",
    "# Paths for the four categories (update these as needed)\n",
    "healthy_spiral_dir = \"HealthySpiral\"\n",
    "healthy_meander_dir = \"HealthyMeander\"\n",
    "patient_spiral_dir = \"PatientSpiral\"\n",
    "patient_meander_dir = \"PatientMeander\"\n",
    "\n",
    "# Helper function: Extract binary masks for HT and ET from an image\n",
    "def extract_traces(image):\n",
    "    \"\"\"\n",
    "    Given a PIL Image, separate it into Handwriting Trace (HT) and Exam Trace (ET) binary masks.\n",
    "    Returns two boolean numpy arrays: ht_mask, et_mask.\n",
    "    \"\"\"\n",
    "    # Optionally apply a blur to reduce noise (median blur for HT, mean blur for ET as per paper)\n",
    "    # image = image.filter(ImageFilter.MedianFilter(size=5))   # for HT smoothing (if needed)\n",
    "    # image = image.filter(ImageFilter.BLUR)                  # for ET smoothing (if needed)\n",
    "    img_array = np.array(image.convert('RGB'))  # ensure RGB\n",
    "    # Split color channels\n",
    "    R, G, B = img_array[:,:,0], img_array[:,:,1], img_array[:,:,2]\n",
    "    # Threshold for ET (exam template trace - usually black): pixel is part of ET if it is dark in all channels\n",
    "    et_mask = (R < 90) & (G < 90) & (B < 90)    # dark (near black) pixels [oai_citation_attribution:10‡file-ppssohspef7crohxhfnw4j](file://file-PPSsohSpeF7cRoHxHfnW4J#:~:text=I%20represents%20each%20pixel%20,b)\n",
    "    # Threshold for HT (handwritten trace): pixel is part of HT if it is not pure white and not part of ET.\n",
    "    # We include colored/darker strokes while excluding the black template:\n",
    "    ht_mask = (R < 200) & (G < 200) & (B < 200)  # pixel is somewhat dark in at least one channel [oai_citation_attribution:11‡file-ppssohspef7crohxhfnw4j](file://file-PPSsohSpeF7cRoHxHfnW4J#:~:text=Consider%20I%20to%20represent%20a,b)\n",
    "    ht_mask = ht_mask & (~et_mask)              # exclude those identified as ET (black) [oai_citation_attribution:12‡file-ppssohspef7crohxhfnw4j](file://file-PPSsohSpeF7cRoHxHfnW4J#:~:text=three%20color%20channels%3A%20red%2C%20green%2C,b)\n",
    "    # Optional: apply dilation to connect any breaks in ET (template) lines (paper used 4x4 dilation) [oai_citation_attribution:13‡file-ppssohspef7crohxhfnw4j](file://file-PPSsohSpeF7cRoHxHfnW4J#:~:text=%7B%200%20if%20I%28r%29%20,90)\n",
    "    et_mask = dilation(et_mask, square(4))\n",
    "    # Ensure boolean type\n",
    "    ht_mask = ht_mask.astype(bool)\n",
    "    et_mask = et_mask.astype(bool)\n",
    "    return ht_mask, et_mask\n",
    "\n",
    "# Load images and assign labels and grouping by patient\n",
    "data_images = []    # list of feature vectors for each image\n",
    "data_labels = []    # list of class labels (0=Healthy, 1=PD) for each image\n",
    "data_patient_ids = []  # list of patient IDs corresponding to each image (for grouping)\n",
    "# Regex to identify patient (or exam) ID from filename\n",
    "id_pattern = re.compile(r'^(\\d+)-')  # matches numeric ID at start of filename (before hyphen)\n",
    "\n",
    "# Process each category folder\n",
    "categories = [\n",
    "    (healthy_spiral_dir, 0), (healthy_meander_dir, 0),\n",
    "    (patient_spiral_dir, 1), (patient_meander_dir, 1)\n",
    "]\n",
    "for folder, label in categories:\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            image = Image.open(img_path).convert('L')  # convert to grayscale (L mode)\n",
    "            # Extract HT and ET traces\n",
    "            ht_mask, et_mask = extract_traces(Image.open(img_path))\n",
    "            # Determine a patient/exam ID from filename for grouping\n",
    "            m = id_pattern.match(filename)\n",
    "            if m:\n",
    "                patient_id = f\"ID_{m.group(1)}\"\n",
    "            else:\n",
    "                # Fallback: use the first numeric sequence in filename (or filename itself if none)\n",
    "                nums = re.findall(r'\\d+', filename)\n",
    "                patient_id = f\"ID_{nums[0]}\" if nums else filename\n",
    "            # Store image data and labels for feature extraction\n",
    "            data_patient_ids.append(patient_id)\n",
    "            data_labels.append(label)\n",
    "            # We will compute features in the next section after obtaining the trace masks\n",
    "            data_images.append((ht_mask, et_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Helper: skeletonize mask and extract ordered path of coordinates\n",
    "def skeleton_to_path(mask):\n",
    "    \"\"\"Convert a binary mask to a skeleton and return an ordered list of (y,x) coordinates along the skeleton.\"\"\"\n",
    "    # Skeletonize to one-pixel width trace\n",
    "    skel = skeletonize(mask)\n",
    "    coords = np.argwhere(skel)\n",
    "    coords_set = {tuple(p) for p in coords}\n",
    "    if not coords_set:\n",
    "        return []\n",
    "    # Identify largest connected component in skeleton (in case of any stray pixels)\n",
    "    # We use BFS to find connected components\n",
    "    largest_comp = set()\n",
    "    while coords_set:\n",
    "        comp = set()\n",
    "        stack = [coords_set.pop()]\n",
    "        comp.add(stack[0])\n",
    "        while stack:\n",
    "            cy, cx = stack.pop()\n",
    "            # check 8-connected neighbors\n",
    "            for dy in (-1,0,1):\n",
    "                for dx in (-1,0,1):\n",
    "                    if dy == 0 and dx == 0:\n",
    "                        continue\n",
    "                    ny, nx = cy+dy, cx+dx\n",
    "                    if (ny, nx) in coords_set:\n",
    "                        coords_set.remove((ny, nx))\n",
    "                        comp.add((ny, nx))\n",
    "                        stack.append((ny, nx))\n",
    "        if len(comp) > len(largest_comp):\n",
    "            largest_comp = comp\n",
    "    coords_list = list(largest_comp)\n",
    "    # Find endpoints (points with only one neighbor in skeleton)\n",
    "    neighbors8 = [(-1,-1),(-1,0),(-1,1),(0,-1),(0,1),(1,-1),(1,0),(1,1)]\n",
    "    end_points = []\n",
    "    for p in coords_list:\n",
    "        count = 0\n",
    "        for dy, dx in neighbors8:\n",
    "            if (p[0]+dy, p[1]+dx) in largest_comp:\n",
    "                count += 1\n",
    "        if count == 1:\n",
    "            end_points.append(p)\n",
    "    # Pick a start point (an endpoint if available, else arbitrary)\n",
    "    start = end_points[0] if end_points else coords_list[0]\n",
    "    # Order the skeleton points by walking from the start\n",
    "    ordered_path = [start]\n",
    "    visited = {tuple(start)}\n",
    "    current = tuple(start)\n",
    "    while True:\n",
    "        next_pt = None\n",
    "        for dy, dx in neighbors8:\n",
    "            nbr = (current[0]+dy, current[1]+dx)\n",
    "            if nbr in largest_comp and nbr not in visited:\n",
    "                next_pt = nbr\n",
    "                break\n",
    "        if next_pt is None:\n",
    "            break\n",
    "        ordered_path.append(next_pt)\n",
    "        visited.add(next_pt)\n",
    "        current = next_pt\n",
    "    return ordered_path\n",
    "\n",
    "# Helper: resample path to N points evenly spaced by arc length\n",
    "def resample_path(path, N):\n",
    "    \"\"\"Resample a polyline (list of (y,x) points) to have N points evenly spaced by arc length.\"\"\"\n",
    "    if len(path) == 0:\n",
    "        return []\n",
    "    # Compute cumulative distances along the path\n",
    "    cumd = [0.0]\n",
    "    for i in range(1, len(path)):\n",
    "        y0,x0 = path[i-1]\n",
    "        y1,x1 = path[i]\n",
    "        dist = math.hypot(x1-x0, y1-y0)\n",
    "        cumd.append(cumd[-1] + dist)\n",
    "    total_length = cumd[-1]\n",
    "    if total_length == 0:\n",
    "        # All points the same\n",
    "        return [path[0]] * N\n",
    "    # sample target distances\n",
    "    target_distances = [i*total_length/(N-1) for i in range(N)]\n",
    "    target_distances[-1] = total_length  # ensure last exactly matches\n",
    "    resampled = []\n",
    "    j = 0\n",
    "    for td in target_distances:\n",
    "        # advance j until cumd[j] <= td < cumd[j+1]\n",
    "        while j < len(cumd)-1 and cumd[j] < td:\n",
    "            j += 1\n",
    "        if cumd[j] == td or j == 0:\n",
    "            # exact match or at start\n",
    "            y, x = path[j]\n",
    "        else:\n",
    "            # interpolate between j-1 and j\n",
    "            y0,x0 = path[j-1]; d0 = cumd[j-1]\n",
    "            y1,x1 = path[j];   d1 = cumd[j]\n",
    "            if d1 - d0 > 0:\n",
    "                t = (td - d0) / (d1 - d0)\n",
    "            else:\n",
    "                t = 0\n",
    "            y = y0 + t * (y1 - y0)\n",
    "            x = x0 + t * (x1 - x0)\n",
    "        resampled.append((y, x))\n",
    "    return resampled\n",
    "\n",
    "# Compute features for each image\n",
    "feature_matrix = []\n",
    "for (ht_mask, et_mask), patient_id in zip(data_images, data_patient_ids):\n",
    "    # Skeletonize and extract ordered trace paths\n",
    "    ht_path = skeleton_to_path(ht_mask)\n",
    "    et_path = skeleton_to_path(et_mask)\n",
    "    # If either path is empty (e.g., failed extraction), skip this image\n",
    "    if len(ht_path) < 2 or len(et_path) < 2:\n",
    "        feature_matrix.append([0]*9)  # placeholder (or continue to skip)\n",
    "        continue\n",
    "    # Resample both traces to the same number of points (based on the longer trace for resolution)\n",
    "    N = max(len(ht_path), len(et_path), 300)  # use at least 300 points for resolution\n",
    "    ht_path_res = resample_path(ht_path, N)\n",
    "    et_path_res = resample_path(et_path, N)\n",
    "    # Compute center of shape (we use the bounding box center of ET as approximation of shape center)\n",
    "    et_coords = np.array(et_path_res)\n",
    "    min_y, max_y = et_coords[:,0].min(), et_coords[:,0].max()\n",
    "    min_x, max_x = et_coords[:,1].min(), et_coords[:,1].max()\n",
    "    center_y, center_x = (min_y + max_y)/2.0, (min_x + max_x)/2.0\n",
    "    # Compute radius for each sampled point\n",
    "    radii_ht = []\n",
    "    radii_et = []\n",
    "    for (y_ht, x_ht), (y_et, x_et) in zip(ht_path_res, et_path_res):\n",
    "        r_ht = math.hypot(x_ht - center_x, y_ht - center_y)\n",
    "        r_et = math.hypot(x_et - center_x, y_et - center_y)\n",
    "        radii_ht.append(r_ht)\n",
    "        radii_et.append(r_et)\n",
    "    radii_ht = np.array(radii_ht)\n",
    "    radii_et = np.array(radii_et)\n",
    "    diff = radii_ht - radii_et\n",
    "    # Feature F1: RMS difference between radii\n",
    "    F1 = math.sqrt(np.mean(diff**2))  # RMS [oai_citation_attribution:27‡file-ppssohspef7crohxhfnw4j](file://file-PPSsohSpeF7cRoHxHfnW4J#:~:text=F1%3A%20The%20root%20mean%20square,as%20the%20length%20of%20the)\n",
    "    # F2: max |difference|\n",
    "    F2 = np.max(np.abs(diff))\n",
    "    # F3: min |difference|\n",
    "    F3 = np.min(np.abs(diff))\n",
    "    # F4: standard deviation of differences\n",
    "    F4 = np.std(diff, ddof=0)\n",
    "    # F5: Mean Relative Tremor (mean |r_HT[i] - r_HT[i-D]|, D=10) [oai_citation_attribution:28‡file-ppssohspef7crohxhfnw4j](file://file-PPSsohSpeF7cRoHxHfnW4J#:~:text=individual%E2%80%99s%20HT,imizing%20the%20PD%20detection%20rate) [oai_citation_attribution:29‡file-ppssohspef7crohxhfnw4j](file://file-PPSsohSpeF7cRoHxHfnW4J#:~:text=%28%7Cr%20i)\n",
    "    D = 10\n",
    "    if len(radii_ht) > D:\n",
    "        tremor_diffs = []\n",
    "        for i in range(D, len(radii_ht)):\n",
    "            tremor_diffs.append(abs(radii_ht[i] - radii_ht[i-D]))\n",
    "        F5 = np.mean(tremor_diffs)\n",
    "    else:\n",
    "        F5 = 0.0\n",
    "    # F6: max ET radius\n",
    "    F6 = np.max(radii_et)\n",
    "    # F7: min ET radius\n",
    "    F7 = np.min(radii_et)\n",
    "    # F8: standard deviation of HT radii\n",
    "    F8 = np.std(radii_ht, ddof=0)\n",
    "    # F9: count of sign changes in (r_HT - r_ET) [oai_citation_attribution:30‡file-ppssohspef7crohxhfnw4j](file://file-PPSsohSpeF7cRoHxHfnW4J#:~:text=F8%3A%20Standard%20Deviation%20of%20ET,values)\n",
    "    sign_diff = np.sign(diff)\n",
    "    # Treat zeros as no sign for change counting\n",
    "    sign_diff[sign_diff==0] = 1  # or could also skip zeros\n",
    "    changes = 0\n",
    "    for i in range(1, len(sign_diff)):\n",
    "        if sign_diff[i] != sign_diff[i-1]:\n",
    "            changes += 1\n",
    "    F9 = changes\n",
    "    features = [F1, F2, F3, F4, F5, F6, F7, F8, F9]\n",
    "    feature_matrix.append(features)\n",
    "\n",
    "feature_matrix = np.array(feature_matrix, dtype=float)\n",
    "\n",
    "# Normalize features (z-score normalization per feature)\n",
    "feat_mean = feature_matrix.mean(axis=0)\n",
    "feat_std = feature_matrix.std(axis=0)\n",
    "feature_matrix_norm = (feature_matrix - feat_mean) / (feat_std + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Prepare dataset grouping by patient for train-test split\n",
    "patient_ids = np.array(data_patient_ids)\n",
    "labels = np.array(data_labels)\n",
    "unique_patients = np.unique(patient_ids)\n",
    "# Determine a label per patient (all images of a patient share same class label)\n",
    "patient_label_map = {}\n",
    "for pid in unique_patients:\n",
    "    # label of first occurrence of this patient\n",
    "    pid_label = labels[patient_ids == pid][0]\n",
    "    patient_label_map[pid] = pid_label\n",
    "patient_labels = np.array([patient_label_map[pid] for pid in unique_patients])\n",
    "\n",
    "# Train-test split at patient level (15% patients for test)\n",
    "# Handle case where we have classes with only one sample - don't use stratify in that case\n",
    "if np.min(np.bincount(patient_labels)) < 2:\n",
    "    train_pids, test_pids = train_test_split(unique_patients, test_size=0.15, random_state=42)\n",
    "    print(\"Warning: Using non-stratified split because some classes have only one sample\")\n",
    "else:\n",
    "    train_pids, test_pids = train_test_split(unique_patients, test_size=0.15, stratify=patient_labels, random_state=42)\n",
    "train_mask = np.isin(patient_ids, train_pids)\n",
    "test_mask  = np.isin(patient_ids, test_pids)\n",
    "X_train = feature_matrix_norm[train_mask]\n",
    "y_train = labels[train_mask]\n",
    "X_test = feature_matrix_norm[test_mask]\n",
    "y_test = labels[test_mask]\n",
    "\n",
    "# Define hyperparameter grids for each model\n",
    "# SVM with RBF (and try linear, poly for completeness) – tune kernel, C, gamma, class_weight\n",
    "svm_param_grid = [\n",
    "    {'kernel': ['linear'], \n",
    "     'C': [0.1, 1, 10, 100], \n",
    "     'class_weight': [None, 'balanced']},\n",
    "    {'kernel': ['rbf', 'poly'], \n",
    "     'C': [0.1, 1, 10, 100], \n",
    "     'gamma': ['scale', 'auto', 0.01, 0.1, 1], \n",
    "     'class_weight': [None, 'balanced']}\n",
    "]\n",
    "svm = SVC(probability=True)  # SVM model (we enable probability estimates for thresholding/AUC)\n",
    "svm_grid_search = GridSearchCV(svm, svm_param_grid, cv=10, scoring='accuracy', n_jobs=-1)\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "best_svm = svm_grid_search.best_estimator_\n",
    "\n",
    "# Logistic Regression with Elastic Net – tune C, l1_ratio, class_weight, fit_intercept\n",
    "logreg_param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'l1_ratio': [0.25, 0.5, 0.75, 1.0],  # 1.0 = L1 (lasso), 0.0 = L2 (ridge)\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'fit_intercept': [True, False],\n",
    "    'solver': ['saga'],\n",
    "    'penalty': ['elasticnet']\n",
    "}\n",
    "logreg = LogisticRegression(max_iter=10000)\n",
    "logreg_grid_search = GridSearchCV(logreg, logreg_param_grid, cv=10, scoring='accuracy', n_jobs=-1)\n",
    "logreg_grid_search.fit(X_train, y_train)\n",
    "best_logreg = logreg_grid_search.best_estimator_\n",
    "\n",
    "print(\"Best SVM parameters:\", svm_grid_search.best_params_)\n",
    "print(\"Best Logistic Regression parameters:\", logreg_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, precision_score\n",
    "\n",
    "# 1. Evaluate on test images (image-level) – this part is fine\n",
    "y_pred_svm = best_svm.predict(X_test)\n",
    "y_pred_lr = best_logreg.predict(X_test)\n",
    "y_prob_svm = best_svm.predict_proba(X_test)[:,1]  # Probability of class 1\n",
    "y_prob_lr = best_logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "auc_svm = roc_auc_score(y_test, y_prob_svm)\n",
    "auc_lr = roc_auc_score(y_test, y_prob_lr)\n",
    "tn_svm, fp_svm, fn_svm, tp_svm = confusion_matrix(y_test, y_pred_svm).ravel()\n",
    "tn_lr, fp_lr, fn_lr, tp_lr = confusion_matrix(y_test, y_pred_lr).ravel()\n",
    "\n",
    "sens_svm = tp_svm / (tp_svm + fn_svm) if tp_svm+fn_svm > 0 else 0.0\n",
    "spec_svm = tn_svm / (tn_svm + fp_svm) if tn_svm+fp_svm > 0 else 0.0\n",
    "sens_lr = tp_lr / (tp_lr + fn_lr) if tp_lr+fn_lr > 0 else 0.0\n",
    "spec_lr = tn_lr / (tn_lr + fp_lr) if tn_lr+fp_lr > 0 else 0.0\n",
    "ppv_svm = precision_score(y_test, y_pred_svm, pos_label=1)\n",
    "ppv_lr = precision_score(y_test, y_pred_lr, pos_label=1)\n",
    "\n",
    "print(f\"SVM Image-level Accuracy: {acc_svm:.3f}, AUC: {auc_svm:.3f}, Sensitivity: {sens_svm:.3f}, Specificity: {spec_svm:.3f}\")\n",
    "print(f\"LogReg Image-level Accuracy: {acc_lr:.3f}, AUC: {auc_lr:.3f}, Sensitivity: {sens_lr:.3f}, Specificity: {spec_lr:.3f}\")\n",
    "\n",
    "# 2. Build a map from global indices to local test indices\n",
    "test_indices = np.where(test_mask)[0]  # global indices for test samples\n",
    "global_to_test_index = {g_idx: i for i, g_idx in enumerate(test_indices)}\n",
    "\n",
    "# 3. Aggregate to patient-level by majority vote\n",
    "test_patient_preds_svm = {}\n",
    "test_patient_preds_lr = {}\n",
    "\n",
    "for pid in test_pids:\n",
    "    true_label = patient_label_map[pid]\n",
    "    # these are global indices\n",
    "    img_indices = np.where((patient_ids == pid) & test_mask)[0]\n",
    "    if len(img_indices) == 0:\n",
    "        continue\n",
    "    # convert to local test indices\n",
    "    local_indices = [global_to_test_index[g_idx] for g_idx in img_indices]\n",
    "    \n",
    "    # count votes\n",
    "    pred_votes_svm = np.sum(y_pred_svm[local_indices] == 1)\n",
    "    pred_votes_lr = np.sum(y_pred_lr[local_indices] == 1)\n",
    "    \n",
    "    # majority vote\n",
    "    patient_pred_svm = 1 if pred_votes_svm > (len(local_indices) / 2) else 0\n",
    "    patient_pred_lr = 1 if pred_votes_lr > (len(local_indices) / 2) else 0\n",
    "    \n",
    "    test_patient_preds_svm[pid] = (true_label, patient_pred_svm)\n",
    "    test_patient_preds_lr[pid] = (true_label, patient_pred_lr)\n",
    "\n",
    "# 4. Calculate patient-level accuracy and confusion\n",
    "tp_svm = fp_svm = tn_svm = fn_svm = 0\n",
    "tp_lr = fp_lr = tn_lr = fn_lr = 0\n",
    "\n",
    "for pid, (true_label, pred_label) in test_patient_preds_svm.items():\n",
    "    if true_label == 1 and pred_label == 1:\n",
    "        tp_svm += 1\n",
    "    elif true_label == 1 and pred_label == 0:\n",
    "        fn_svm += 1\n",
    "    elif true_label == 0 and pred_label == 1:\n",
    "        fp_svm += 1\n",
    "    elif true_label == 0 and pred_label == 0:\n",
    "        tn_svm += 1\n",
    "\n",
    "for pid, (true_label, pred_label) in test_patient_preds_lr.items():\n",
    "    if true_label == 1 and pred_label == 1:\n",
    "        tp_lr += 1\n",
    "    elif true_label == 1 and pred_label == 0:\n",
    "        fn_lr += 1\n",
    "    elif true_label == 0 and pred_label == 1:\n",
    "        fp_lr += 1\n",
    "    elif true_label == 0 and pred_label == 0:\n",
    "        tn_lr += 1\n",
    "\n",
    "patient_acc_svm = (tp_svm + tn_svm) / (tp_svm+tn_svm+fp_svm+fn_svm)\n",
    "patient_acc_lr = (tp_lr + tn_lr) / (tp_lr+tn_lr+fp_lr+fn_lr)\n",
    "\n",
    "sens_svm = tp_svm / (tp_svm + fn_svm) if (tp_svm+fn_svm) else 0\n",
    "spec_svm = tn_svm / (tn_svm + fp_svm) if (tn_svm+fp_svm) else 0\n",
    "sens_lr = tp_lr / (tp_lr + fn_lr) if (tp_lr+fn_lr) else 0\n",
    "spec_lr = tn_lr / (tn_lr + fp_lr) if (tn_lr+fp_lr) else 0\n",
    "\n",
    "print(f\"SVM Patient-level Accuracy: {patient_acc_svm:.3f}, Sensitivity: {sens_svm:.3f}, Specificity: {spec_svm:.3f}\")\n",
    "print(f\"LogReg Patient-level Accuracy: {patient_acc_lr:.3f}, Sensitivity: {sens_lr:.3f}, Specificity: {spec_lr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate combined models on spiral vs meander test images\n",
    "# Identify test indices for spirals and meanders (assuming file path or patient_id can tell shape; here we use the folder info stored in mask arrays)\n",
    "# We know the order in data_images corresponds to categories in the same order we loaded.\n",
    "test_idx = np.where(test_mask)[0]\n",
    "# Reconstruct shape type from original category loading order (we appended in known sequence: healthy_spiral, healthy_meander, patient_spiral, patient_meander)\n",
    "shapes = []\n",
    "for idx in test_idx:\n",
    "    # We can infer shape by the index position from loading sequence:\n",
    "    # (Not elegant; ideally file names or a separate array indicating shape)\n",
    "    # For simplicity, assume if file came from a spiral folder, we tagged it in its name or order.\n",
    "    # Here, we'll just check the corresponding original mask tuple in data_images: \n",
    "    # if it came from healthy_spiral_dir or patient_spiral_dir, it was loaded before any meander in each half.\n",
    "    shapes.append('spiral' if idx < len(os.listdir(healthy_spiral_dir)) + len(os.listdir(patient_spiral_dir)) else 'meander')\n",
    "# Now compute accuracy for each shape subset\n",
    "spiral_idx = [i for i,sh in zip(test_idx, shapes) if sh == 'spiral']\n",
    "meander_idx = [i for i,sh in zip(test_idx, shapes) if sh == 'meander']\n",
    "acc_svm_spiral = accuracy_score(y_test[[np.where(test_idx==i)[0][0] for i in spiral_idx]], y_pred_svm[[np.where(test_idx==i)[0][0] for i in spiral_idx]]) if spiral_idx else None\n",
    "acc_svm_meander = accuracy_score(y_test[[np.where(test_idx==i)[0][0] for i in meander_idx]], y_pred_svm[[np.where(test_idx==i)[0][0] for i in meander_idx]]) if meander_idx else None\n",
    "acc_lr_spiral = accuracy_score(y_test[[np.where(test_idx==i)[0][0] for i in spiral_idx]], y_pred_lr[[np.where(test_idx==i)[0][0] for i in spiral_idx]]) if spiral_idx else None\n",
    "acc_lr_meander = accuracy_score(y_test[[np.where(test_idx==i)[0][0] for i in meander_idx]], y_pred_lr[[np.where(test_idx==i)[0][0] for i in meander_idx]]) if meander_idx else None\n",
    "\n",
    "print(f\"SVM Accuracy on Spiral drawings: {acc_svm_spiral:.3f}, on Meander drawings: {acc_svm_meander:.3f}\")\n",
    "print(f\"LogReg Accuracy on Spiral drawings: {acc_lr_spiral:.3f}, on Meander drawings: {acc_lr_meander:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
