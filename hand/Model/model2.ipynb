{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived cluster centers (feature space): [[8.20579783e-02 4.52367605e-02 8.78108369e-01 9.20882751e-01\n",
      "  4.56386293e+00]\n",
      " [3.90545281e-02 4.62636752e-02 9.09406029e-01 9.76526408e-01\n",
      "  1.20500000e+02]\n",
      " [5.41301437e-02 3.53160399e-02 8.56267489e-01 8.95567030e-01\n",
      "  4.14090909e+01]\n",
      " [6.85996969e-02 4.43187102e-02 8.79246636e-01 9.27294871e-01\n",
      "  1.71824818e+01]\n",
      " [2.27281220e-02 2.27012175e-02 8.67535856e-01 9.19010494e-01\n",
      "  7.29375000e+01]]\n",
      "Assigned severity label counts: [321  10  44 137  16]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Paths and categories\n",
    "categories = [\"HealthySpiral\", \"HealthyMeander\", \"PatientSpiral\", \"PatientMeander\"]\n",
    "\n",
    "X_images = []      # list to store image data for CNN\n",
    "X_features = []    # list to store extracted feature vectors\n",
    "initial_labels = []  # healthy/patient label (optional, for analysis)\n",
    "\n",
    "# Loop over each category folder\n",
    "for category in categories:\n",
    "    folder_path = category\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            # Load image in color and grayscale\n",
    "            img_color = cv2.imread(img_path)  # BGR color image\n",
    "            gray = cv2.cvtColor(img_color, cv2.COLOR_BGR2GRAY)\n",
    "            # Resize images to a standard size (e.g., 224x224) for consistency\n",
    "            img_color = cv2.resize(img_color, (224, 224))\n",
    "            gray = cv2.resize(gray, (224, 224))\n",
    "            # Append to image list (for CNN) and extract features for feature list\n",
    "            X_images.append(img_color)\n",
    "            features = []  # will be filled by feature extraction\n",
    "            \n",
    "            # Feature 1: Size (normalized area of drawing)\n",
    "            # Invert image to get drawing in white on black for contour detection\n",
    "            _, bw = cv2.threshold(cv2.bitwise_not(gray), 128, 255, cv2.THRESH_BINARY)\n",
    "            contours, _ = cv2.findContours(bw, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            if len(contours) == 0:\n",
    "                # If no contour found (blank image), skip or append zero features\n",
    "                features = [0, 0, 0, 0, 0]\n",
    "            else:\n",
    "                cnt = max(contours, key=cv2.contourArea)  # largest contour\n",
    "                area = cv2.contourArea(cnt)\n",
    "                norm_area = area / float(gray.shape[0] * gray.shape[1])\n",
    "                # Feature 2: Stroke length (contour perimeter normalized)\n",
    "                perimeter = cv2.arcLength(cnt, closed=True)\n",
    "                norm_perimeter = perimeter / float(gray.shape[0] * gray.shape[1])\n",
    "                # Compute angles along the contour to measure smoothness/tremor\n",
    "                angles = []\n",
    "                for i in range(len(cnt) - 1):\n",
    "                    p1 = cnt[i][0]\n",
    "                    p2 = cnt[i+1][0]\n",
    "                    dx = p2[0] - p1[0]\n",
    "                    dy = p2[1] - p1[1]\n",
    "                    angles.append(np.arctan2(dy, dx))\n",
    "                angles = np.unwrap(angles)  # unwrap angles to avoid discontinuities\n",
    "                if len(angles) >= 2:\n",
    "                    angle_diffs = np.diff(angles)\n",
    "                else:\n",
    "                    angle_diffs = np.array([0.0])\n",
    "                # Feature 3: Smoothness (mean absolute change in angle)\n",
    "                smoothness = float(np.mean(np.abs(angle_diffs)))\n",
    "                # Feature 4: Tremor (variability of angle changes)\n",
    "                tremor = float(np.std(angle_diffs))\n",
    "                # Feature 5: Number of stroke segments (number of contours)\n",
    "                stroke_count = len(contours)\n",
    "                features = [norm_area, norm_perimeter, smoothness, tremor, stroke_count]\n",
    "            X_features.append(features)\n",
    "            # Record initial group label (not severity, just healthy/patient) if needed\n",
    "            initial_labels.append(0 if \"Healthy\" in category else 1)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_images = np.array(X_images)         # shape (N, 224, 224, 3)\n",
    "X_features = np.array(X_features)     # shape (N, feature_dim)\n",
    "initial_labels = np.array(initial_labels)\n",
    "\n",
    "# Apply K-Means clustering to features to derive 5 severity clusters (0–4)\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "severity_labels = kmeans.fit_predict(X_features)  # cluster assignment for each sample\n",
    "\n",
    "# Optionally, map clusters to severity scale by ordering cluster centroids \n",
    "# (e.g., cluster with most healthy samples -> 0, most extreme tremor -> 4).\n",
    "# For simplicity, we use the cluster indices directly as severity labels.\n",
    "print(\"Derived cluster centers (feature space):\", kmeans.cluster_centers_)\n",
    "print(\"Assigned severity label counts:\", np.bincount(severity_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Classification Accuracy (test): 99.06%\n",
      "k-NN Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        64\n",
      "           1       1.00      0.50      0.67         2\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        28\n",
      "           4       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.99       106\n",
      "   macro avg       0.95      0.90      0.90       106\n",
      "weighted avg       0.99      0.99      0.99       106\n",
      "\n",
      "SVM Classification Accuracy (test): 96.23%\n",
      "SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        64\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       0.86      0.67      0.75         9\n",
      "           3       0.90      1.00      0.95        28\n",
      "           4       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.96       106\n",
      "   macro avg       0.95      0.87      0.90       106\n",
      "weighted avg       0.96      0.96      0.96       106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split data into train and test sets (e.g., 80% train, 20% test)\n",
    "X_feat_train, X_feat_test, X_img_train, X_img_test, y_train, y_test = train_test_split(\n",
    "    X_features, X_images, severity_labels, test_size=0.2, random_state=42, stratify=severity_labels\n",
    ")\n",
    "\n",
    "# --- k-Nearest Neighbors (k-NN) ---\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_feat_train, y_train)\n",
    "y_pred_knn = knn.predict(X_feat_test)\n",
    "\n",
    "print(\"k-NN Classification Accuracy (test): {:.2f}%\".format(100 * accuracy_score(y_test, y_pred_knn)))\n",
    "print(\"k-NN Classification Report:\\n\", classification_report(y_test, y_pred_knn))\n",
    "\n",
    "# --- Support Vector Machine (SVM) ---\n",
    "svm = SVC(kernel='rbf', C=1.0, probability=True, random_state=42)\n",
    "svm.fit(X_feat_train, y_train)\n",
    "y_pred_svm = svm.predict(X_feat_test)\n",
    "\n",
    "print(\"SVM Classification Accuracy (test): {:.2f}%\".format(100 * accuracy_score(y_test, y_pred_svm)))\n",
    "print(\"SVM Classification Report:\\n\", classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# (Optional) Hyperparameter tuning for SVM using cross-validation\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly']}\n",
    "# grid = GridSearchCV(SVC(probability=True), param_grid, cv=5)\n",
    "# grid.fit(X_feat_train, y_train)\n",
    "# best_svm = grid.best_estimator_\n",
    "# print(\"Best SVM parameters:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 369ms/step - accuracy: 0.4810 - loss: 1.3677 - val_accuracy: 0.6512 - val_loss: 0.9319\n",
      "Epoch 2/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 358ms/step - accuracy: 0.6026 - loss: 1.1083 - val_accuracy: 0.6512 - val_loss: 0.9150\n",
      "Epoch 3/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 380ms/step - accuracy: 0.5934 - loss: 1.1094 - val_accuracy: 0.6512 - val_loss: 0.9080\n",
      "Epoch 4/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 343ms/step - accuracy: 0.5974 - loss: 1.1432 - val_accuracy: 0.6512 - val_loss: 0.8947\n",
      "Epoch 5/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 362ms/step - accuracy: 0.6042 - loss: 1.0733 - val_accuracy: 0.6512 - val_loss: 0.8961\n",
      "Epoch 6/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 394ms/step - accuracy: 0.5953 - loss: 1.0689 - val_accuracy: 0.6512 - val_loss: 0.9279\n",
      "Epoch 7/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 347ms/step - accuracy: 0.6200 - loss: 1.0579 - val_accuracy: 0.6512 - val_loss: 0.9000\n",
      "Epoch 8/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 345ms/step - accuracy: 0.6142 - loss: 1.0626 - val_accuracy: 0.6512 - val_loss: 0.8954\n",
      "Epoch 9/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 337ms/step - accuracy: 0.6062 - loss: 1.0344 - val_accuracy: 0.6512 - val_loss: 0.9041\n",
      "Epoch 10/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 344ms/step - accuracy: 0.5720 - loss: 1.0824 - val_accuracy: 0.6512 - val_loss: 0.8795\n",
      "Epoch 11/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 340ms/step - accuracy: 0.6064 - loss: 1.0811 - val_accuracy: 0.6512 - val_loss: 0.8987\n",
      "Epoch 12/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 339ms/step - accuracy: 0.6192 - loss: 1.0123 - val_accuracy: 0.6512 - val_loss: 0.9196\n",
      "Epoch 13/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 339ms/step - accuracy: 0.6153 - loss: 1.0395 - val_accuracy: 0.6512 - val_loss: 0.9032\n",
      "Epoch 14/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 338ms/step - accuracy: 0.5812 - loss: 1.0612 - val_accuracy: 0.6512 - val_loss: 0.8656\n",
      "Epoch 15/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 340ms/step - accuracy: 0.5850 - loss: 1.0949 - val_accuracy: 0.6512 - val_loss: 0.8473\n",
      "Epoch 16/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 338ms/step - accuracy: 0.6107 - loss: 1.0123 - val_accuracy: 0.6512 - val_loss: 0.8915\n",
      "Epoch 17/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 336ms/step - accuracy: 0.6378 - loss: 1.0001 - val_accuracy: 0.6512 - val_loss: 0.9127\n",
      "Epoch 18/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 336ms/step - accuracy: 0.6009 - loss: 1.0174 - val_accuracy: 0.6512 - val_loss: 0.8202\n",
      "Epoch 19/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 344ms/step - accuracy: 0.6042 - loss: 1.0119 - val_accuracy: 0.6512 - val_loss: 0.8369\n",
      "Epoch 20/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 338ms/step - accuracy: 0.5649 - loss: 1.0530 - val_accuracy: 0.6512 - val_loss: 0.7879\n",
      "CNN Test Accuracy: 60.38%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Normalize image pixel values to [0,1]\n",
    "X_img_train = X_img_train.astype('float32') / 255.0\n",
    "X_img_test  = X_img_test.astype('float32')  / 255.0\n",
    "\n",
    "# Simple CNN model definition\n",
    "model_cnn = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    layers.GlobalAveragePooling2D(),  # reduce feature maps to a single 128-d vector\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(5, activation='softmax')  # 5 severity classes\n",
    "])\n",
    "model_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "history = model_cnn.fit(X_img_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = model_cnn.evaluate(X_img_test, y_test, verbose=0)\n",
    "print(\"CNN Test Accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 777ms/step - accuracy: 0.2563 - loss: 1.4527 - val_accuracy: 0.6512 - val_loss: 0.9321\n",
      "Epoch 2/5\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 704ms/step - accuracy: 0.5814 - loss: 1.1046 - val_accuracy: 0.6512 - val_loss: 0.8681\n",
      "Epoch 3/5\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 790ms/step - accuracy: 0.5884 - loss: 1.0412 - val_accuracy: 0.6512 - val_loss: 0.8356\n",
      "Epoch 4/5\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 714ms/step - accuracy: 0.6401 - loss: 0.9057 - val_accuracy: 0.7209 - val_loss: 0.8428\n",
      "Epoch 5/5\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 699ms/step - accuracy: 0.5911 - loss: 1.0448 - val_accuracy: 0.6512 - val_loss: 0.8578\n",
      "Epoch 1/5\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.3284 - loss: 1.5762 - val_accuracy: 0.6512 - val_loss: 0.8528\n",
      "Epoch 2/5\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.7038 - loss: 0.7314 - val_accuracy: 0.6512 - val_loss: 0.8533\n",
      "Epoch 3/5\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.7708 - loss: 0.6115 - val_accuracy: 0.6512 - val_loss: 0.8480\n",
      "Epoch 4/5\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.8211 - loss: 0.5150 - val_accuracy: 0.6744 - val_loss: 0.8539\n",
      "Epoch 5/5\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.8306 - loss: 0.4986 - val_accuracy: 0.6977 - val_loss: 0.8710\n",
      "ResNet50 Transfer Learning Test Accuracy: 62.26%\n"
     ]
    }
   ],
   "source": [
    "# Transfer Learning with ResNet50\n",
    "base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # freeze the convolutional base\n",
    "# Add custom top layers\n",
    "x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output_layer = layers.Dense(5, activation='softmax')(x)\n",
    "model_resnet = tf.keras.Model(inputs=base_model.input, outputs=output_layer)\n",
    "\n",
    "model_resnet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train only the top layer for a few epochs\n",
    "model_resnet.fit(X_img_train, y_train, epochs=5, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Fine-tune: unfreeze some layers of ResNet (e.g., last few blocks) and continue training with a smaller learning rate\n",
    "base_model.trainable = True\n",
    "# Freeze all layers except the last 50 layers (for example)\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "# Recompile with a lower learning rate for fine-tuning\n",
    "model_resnet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                     loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_resnet.fit(X_img_train, y_train, epochs=5, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Evaluate ResNet model on test set\n",
    "resnet_loss, resnet_acc = model_resnet.evaluate(X_img_test, y_test, verbose=0)\n",
    "print(\"ResNet50 Transfer Learning Test Accuracy: {:.2f}%\".format(resnet_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 700ms/step\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x40186d940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 14 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x40186d940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 641ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 488ms/step\n",
      "Combined feature vector size: 2053\n",
      "Ensemble (Feature+CNN) Accuracy (test): 80.19%\n",
      "Ensemble Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91        64\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.57      0.44      0.50         9\n",
      "           3       0.72      0.64      0.68        28\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.80       106\n",
      "   macro avg       0.43      0.41      0.42       106\n",
      "weighted avg       0.75      0.80      0.77       106\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# --- Method 1: Average probabilities from SVM and CNN (late fusion ensemble) ---\n",
    "# Get probability predictions on the test set from SVM and CNN\n",
    "svm_probs = svm.predict_proba(X_feat_test)             # shape (n_samples, 5)\n",
    "cnn_probs = model_resnet.predict(X_img_test)           # shape (n_samples, 5)\n",
    "avg_probs = (svm_probs + cnn_probs) / 2\n",
    "ensemble_pred_labels = np.argmax(avg_probs, axis=1)\n",
    "\n",
    "# --- Method 2: Feature fusion and new classifier (early fusion ensemble) ---\n",
    "# Extract CNN embeddings (penultimate layer outputs) for train and test images\n",
    "# For the ResNet model, we already have a global pooling layer output of size 2048 (before Dense).\n",
    "# We can create a new model that outputs that layer (x) from model_resnet.\n",
    "embed_model = tf.keras.Model(inputs=model_resnet.input, outputs=base_model.output)  # get conv base output (7x7x2048)\n",
    "# Actually, better to get the pooled output directly:\n",
    "embed_model = tf.keras.Model(inputs=model_resnet.input, outputs=model_resnet.layers[-2].output)  # output of GlobalAveragePooling2D\n",
    "train_embeds = embed_model.predict(X_img_train)  # shape (n_train, 2048)\n",
    "test_embeds = embed_model.predict(X_img_test)    # shape (n_test, 2048)\n",
    "\n",
    "# Concatenate manual features with CNN embeddings\n",
    "combined_train_features = np.concatenate([X_feat_train, train_embeds], axis=1)\n",
    "combined_test_features  = np.concatenate([X_feat_test,  test_embeds],  axis=1)\n",
    "print(\"Combined feature vector size:\", combined_train_features.shape[1])\n",
    "\n",
    "# Train an ensemble classifier on the combined features (e.g., Random Forest)\n",
    "ensemble_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "ensemble_clf.fit(combined_train_features, y_train)\n",
    "ensemble_pred = ensemble_clf.predict(combined_test_features)\n",
    "\n",
    "# Evaluate the hybrid model\n",
    "ensemble_acc = accuracy_score(y_test, ensemble_pred)\n",
    "print(\"Ensemble (Feature+CNN) Accuracy (test): {:.2f}%\".format(ensemble_acc * 100))\n",
    "print(\"Ensemble Classification Report:\\n\", classification_report(y_test, ensemble_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (rows=true, cols=pred):\n",
      " [[63  0  0  1  0]\n",
      " [ 0  0  0  2  0]\n",
      " [ 1  0  4  4  0]\n",
      " [10  0  0 18  0]\n",
      " [ 0  0  3  0  0]]\n",
      "Sensitivity (Recall) for classes 0-4: [0.984 0.    0.444 0.643 0.   ]\n",
      "Specificity for classes 0-4: [0.738 1.    0.969 0.91  1.   ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion matrix for ensemble predictions\n",
    "cm = confusion_matrix(y_test, ensemble_pred)\n",
    "print(\"Confusion Matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "# Calculate sensitivity (recall) and specificity for each class\n",
    "num_classes = cm.shape[0]\n",
    "sensitivity = np.zeros(num_classes)\n",
    "specificity = np.zeros(num_classes)\n",
    "for i in range(num_classes):\n",
    "    TP = cm[i, i]\n",
    "    FN = np.sum(cm[i, :]) - TP\n",
    "    FP = np.sum(cm[:, i]) - TP\n",
    "    TN = np.sum(cm) - (TP + FP + FN)\n",
    "    sensitivity[i] = TP / float(TP + FN) if (TP+FN) > 0 else 0.0\n",
    "    specificity[i] = TN / float(TN + FP) if (TN+FP) > 0 else 0.0\n",
    "\n",
    "print(\"Sensitivity (Recall) for classes 0-4:\", np.round(sensitivity, 3))\n",
    "print(\"Specificity for classes 0-4:\", np.round(specificity, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Save models to disk\n",
    "joblib.dump(svm, \"svm_feature_model.pkl\")\n",
    "joblib.dump(ensemble_clf, \"hybrid_ensemble_model.pkl\")\n",
    "model_resnet.save(\"cnn_severity_model.h5\")\n",
    "\n",
    "# Later, load the models (in deployment environment)\n",
    "svm_loaded = joblib.load(\"svm_feature_model.pkl\")\n",
    "ensemble_loaded = joblib.load(\"hybrid_ensemble_model.pkl\")\n",
    "cnn_loaded = tf.keras.models.load_model(\"cnn_severity_model.h5\")\n",
    "# Recreate the embedding model for CNN (to get penultimate layer output if needed for hybrid)\n",
    "embed_loaded = tf.keras.Model(inputs=cnn_loaded.input, outputs=cnn_loaded.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "Predicted severity: 3\n"
     ]
    }
   ],
   "source": [
    "def predict_severity(image_path):\n",
    "    # 1. Load and preprocess the image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "    img_resized = cv2.resize(img, (224, 224))\n",
    "    gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
    "    # 2. Extract features from the image\n",
    "    _, bw = cv2.threshold(cv2.bitwise_not(gray), 128, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(bw, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "        area = cv2.contourArea(cnt)\n",
    "        norm_area = area / float(gray.shape[0] * gray.shape[1])\n",
    "        perimeter = cv2.arcLength(cnt, True)\n",
    "        norm_perimeter = perimeter / float(gray.shape[0] * gray.shape[1])\n",
    "        angles = []\n",
    "        for i in range(len(cnt) - 1):\n",
    "            p1 = cnt[i][0]; p2 = cnt[i+1][0]\n",
    "            angles.append(np.arctan2(p2[1]-p1[1], p2[0]-p1[0]))\n",
    "        angles = np.unwrap(angles)\n",
    "        angle_diffs = np.diff(angles) if len(angles) >= 2 else np.array([0.0])\n",
    "        smoothness = float(np.mean(np.abs(angle_diffs))) if angle_diffs.size > 0 else 0.0\n",
    "        tremor = float(np.std(angle_diffs)) if angle_diffs.size > 0 else 0.0\n",
    "        stroke_count = len(contours)\n",
    "        feat_vector = np.array([norm_area, norm_perimeter, smoothness, tremor, stroke_count]).reshape(1, -1)\n",
    "    else:\n",
    "        # if no contour found, use zeros\n",
    "        feat_vector = np.zeros((1,5), dtype=float)\n",
    "    # 3. Feature model prediction\n",
    "    feat_pred_proba = svm_loaded.predict_proba(feat_vector)  # probability from SVM\n",
    "    # 4. CNN model prediction\n",
    "    img_input = img_resized.astype('float32')/255.0\n",
    "    img_input = np.expand_dims(img_input, axis=0)  # shape (1,224,224,3)\n",
    "    cnn_pred_proba = cnn_loaded.predict(img_input)\n",
    "    # 5. Combine predictions - average probabilities\n",
    "    avg_proba = (feat_pred_proba + cnn_pred_proba) / 2\n",
    "    final_class = int(np.argmax(avg_proba, axis=1)[0])\n",
    "    # Alternatively, use the hybrid model directly on combined features:\n",
    "    # embed_feat = embed_loaded.predict(img_input)  # get embedding from CNN\n",
    "    # combined_feat = np.concatenate([feat_vector, embed_feat], axis=1)\n",
    "    # final_class = int(ensemble_loaded.predict(combined_feat)[0])\n",
    "    return final_class\n",
    "\n",
    "# Example usage:\n",
    "print(\"Predicted severity:\", predict_severity(\"mea3-P6.jpg\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
